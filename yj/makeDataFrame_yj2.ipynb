{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accafd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import request\n",
    "from requests.compat import *\n",
    "from bs4 import BeautifulSoup\n",
    "from user_agent import generate_user_agent\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def makePageDF(category_id, page_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : str(page_id),\n",
    "        'pagingSize' : '80',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    \n",
    "    # 각 상품의 정보를 저장할 리스트를 생성합니다.\n",
    "    products_info = []\n",
    "\n",
    "    for item in itemList['list'][:80]:\n",
    "\n",
    "        attributes_dict = dict()\n",
    "        # 'attributeValue'와 'characterValue' 문자열을 리스트로 분리합니다.\n",
    "        attribute_values = item['item']['attributeValue'].split('|')\n",
    "        character_values = item['item']['characterValue'].split('|')\n",
    "\n",
    "        # 'attributeValue' 리스트의 각 요소에서 '_M' 문자열을 제거합니다.\n",
    "        attribute_values = [value.replace('_M', '') for value in attribute_values]\n",
    "\n",
    "        min_len = min(len(attribute_values), len(character_values))\n",
    "\n",
    "        for j in range(min_len):\n",
    "            attribute = attribute_values[j]\n",
    "            character = character_values[j]\n",
    "\n",
    "            if attribute in attributes_dict:\n",
    "                attributes_dict[attribute].append(character)\n",
    "            else:\n",
    "                attributes_dict[attribute] = [character]\n",
    "\n",
    "        attribute_list = [item['item']['category3Name']]\n",
    "\n",
    "        for i in attributes_dict:\n",
    "            if i not in ['용량', '구성', '']:\n",
    "                attribute_list = attribute_list + attributes_dict[i]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        dict_data = {\n",
    "            'ID': item['item']['id'],\n",
    "            '상품명': item['item']['productName'],\n",
    "            '상품 카테고리 대분류': item['item']['category1Name'],\n",
    "            '상품 카테고리 중분류': item['item']['category2Name'],\n",
    "            '상품 카테고리 소분류': item['item']['category3Name'],\n",
    "            '제조사': item['item']['maker'],\n",
    "            '브랜드': item['item']['brand'],\n",
    "            '특징': attribute_list\n",
    "        }\n",
    "        for attribute in attribute_list:\n",
    "            dict_data[attribute] = True\n",
    "\n",
    "        products_info.append(dict_data)\n",
    "        \n",
    "        # 딕셔너리의 리스트를 데이터프레임으로 변환합니다.\n",
    "    df = pd.DataFrame(products_info)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def makeCategoryDF(category_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : '1',\n",
    "        'pagingSize' : '80',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    total = data['props']['pageProps']['initialState']['products']['total']\n",
    "    total_page = math.ceil(total/80)\n",
    "    \n",
    "    df = makePageDF(category_id,1)\n",
    "    \n",
    "    for j in range(1,10):\n",
    "        if total_page >1:\n",
    "            for i in range(2,total_page+1):\n",
    "                new_df = makePageDF(category_id,i)\n",
    "                df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['ID'])\n",
    "    df = pd.DataFrame(df).astype({'제조사': 'category'})\n",
    "    df = pd.DataFrame(df).astype({'브랜드': 'category'})\n",
    "    attribute_columns = df.columns.drop(['ID', '상품명', '상품 카테고리 대분류', '상품 카테고리 중분류','상품 카테고리 소분류','제조사','브랜드','특징'])\n",
    "    df[attribute_columns] = df[attribute_columns].fillna(False).astype('bool')\n",
    "    small_df = df.explode('특징')[['ID','상품명','특징']]\n",
    "    attributes = pd.DataFrame(small_df['특징'].unique()).reset_index()\n",
    "    attributes.columns = ['attribute_id', '특징']\n",
    "    attributes = pd.DataFrame(attributes).astype({'attribute_id': 'category'})\n",
    "    merged_df = pd.merge(small_df, attributes, on=['특징'])\n",
    "    \n",
    "    return df, merged_df, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6318299e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df, merged_df, attributes \u001b[38;5;241m=\u001b[39m makeCategoryDF(\u001b[38;5;241m100001011\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m, in \u001b[0;36mmakeCategoryDF\u001b[1;34m(category_id)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_page \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,total_page\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 95\u001b[0m             new_df \u001b[38;5;241m=\u001b[39m makePageDF(category_id,i)\n\u001b[0;32m     96\u001b[0m             df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, new_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     98\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m, in \u001b[0;36mmakePageDF\u001b[1;34m(category_id, page_id)\u001b[0m\n\u001b[0;32m     10\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpagingIndex\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mstr\u001b[39m(page_id),\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpagingSize\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m80\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproductSet\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     15\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m:generate_user_agent(os \u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmac\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinux\u001b[39m\u001b[38;5;124m'\u001b[39m),device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesktop\u001b[39m\u001b[38;5;124m'\u001b[39m)}\n\u001b[1;32m---> 16\u001b[0m resp \u001b[38;5;241m=\u001b[39m request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m,url\u001b[38;5;241m=\u001b[39murl, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m     17\u001b[0m dom \u001b[38;5;241m=\u001b[39m BeautifulSoup(resp\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m script_tag \u001b[38;5;241m=\u001b[39m dom\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__NEXT_DATA__\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:487\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    484\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 487\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    488\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    489\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    490\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    491\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    492\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    496\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    497\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    498\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    499\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df, merged_df, attributes = makeCategoryDF(100001011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71142769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bf705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePageDF(category_id, page_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : str(page_id),\n",
    "        'pagingSize' : '80',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    \n",
    "    # 각 상품의 정보를 저장할 리스트를 생성합니다.\n",
    "    products_info = []\n",
    "\n",
    "    for item in itemList['list'][:80]:\n",
    "\n",
    "        attributes_dict = dict()\n",
    "        # 'attributeValue'와 'characterValue' 문자열을 리스트로 분리합니다.\n",
    "        attribute_values = item['item']['attributeValue'].split('|')\n",
    "        character_values = item['item']['characterValue'].split('|')\n",
    "\n",
    "        # 'attributeValue' 리스트의 각 요소에서 '_M' 문자열을 제거합니다.\n",
    "        attribute_values = [value.replace('_M', '') for value in attribute_values]\n",
    "\n",
    "        min_len = min(len(attribute_values), len(character_values))\n",
    "\n",
    "        for j in range(min_len):\n",
    "            attribute = attribute_values[j]\n",
    "            character = character_values[j]\n",
    "\n",
    "            if attribute in attributes_dict:\n",
    "                attributes_dict[attribute].append(character)\n",
    "            else:\n",
    "                attributes_dict[attribute] = [character]\n",
    "\n",
    "        attribute_list = [item['item']['category3Name']]\n",
    "\n",
    "        for i in attributes_dict:\n",
    "            if i not in ['용량', '구성', '']:\n",
    "                attribute_list = attribute_list + attributes_dict[i]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        dict_data = {\n",
    "            'ID': item['item']['id'],\n",
    "            '상품명': item['item']['productName'],\n",
    "            '상품 카테고리 대분류': item['item']['category1Name'],\n",
    "            '상품 카테고리 중분류': item['item']['category2Name'],\n",
    "            '상품 카테고리 소분류': item['item']['category3Name'],\n",
    "            '제조사': item['item']['maker'],\n",
    "            '브랜드': item['item']['brand'],\n",
    "            '특징': attribute_list\n",
    "        }\n",
    "        for attribute in attribute_list:\n",
    "            dict_data[attribute] = True\n",
    "\n",
    "        products_info.append(dict_data)\n",
    "        \n",
    "        # 딕셔너리의 리스트를 데이터프레임으로 변환합니다.\n",
    "    df = pd.DataFrame(products_info)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def makeCategoryDF(category_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : '1',\n",
    "        'pagingSize' : '80',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    total = data['props']['pageProps']['initialState']['products']['total']\n",
    "    total_page = math.ceil(total/80)\n",
    "    \n",
    "    df = makePageDF(category_id,1)\n",
    "    \n",
    "    for j in range(1,5):\n",
    "        if total_page >1:\n",
    "            for i in range(2,total_page+1):\n",
    "                new_df = makePageDF(category_id,i)\n",
    "                df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['ID'])\n",
    "    df = pd.DataFrame(df).astype({'제조사': 'category'})\n",
    "    df = pd.DataFrame(df).astype({'브랜드': 'category'})\n",
    "    attribute_columns = df.columns.drop(['ID', '상품명', '상품 카테고리 대분류', '상품 카테고리 중분류','상품 카테고리 소분류','제조사','브랜드','특징'])\n",
    "    df[attribute_columns] = df[attribute_columns].fillna(False).astype('bool')\n",
    "    small_df = df.explode('특징')[['ID','상품명','특징']]\n",
    "    attributes = pd.DataFrame(small_df['특징'].unique()).reset_index()\n",
    "    attributes.columns = ['attribute_id', '특징']\n",
    "    attributes = pd.DataFrame(attributes).astype({'attribute_id': 'category'})\n",
    "    merged_df = pd.merge(small_df, attributes, on=['특징'])\n",
    "    \n",
    "    return df, merged_df, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, merged_df, attributes = makeCategoryDF(100001011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ebee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePageDF(category_id, page_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : str(page_id),\n",
    "        'pagingSize' : '40',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    \n",
    "    # 각 상품의 정보를 저장할 리스트를 생성합니다.\n",
    "    products_info = []\n",
    "\n",
    "    for item in itemList['list'][:40]:\n",
    "\n",
    "        attributes_dict = dict()\n",
    "        # 'attributeValue'와 'characterValue' 문자열을 리스트로 분리합니다.\n",
    "        attribute_values = item['item']['attributeValue'].split('|')\n",
    "        character_values = item['item']['characterValue'].split('|')\n",
    "\n",
    "        # 'attributeValue' 리스트의 각 요소에서 '_M' 문자열을 제거합니다.\n",
    "        attribute_values = [value.replace('_M', '') for value in attribute_values]\n",
    "\n",
    "        min_len = min(len(attribute_values), len(character_values))\n",
    "\n",
    "        for j in range(min_len):\n",
    "            attribute = attribute_values[j]\n",
    "            character = character_values[j]\n",
    "\n",
    "            if attribute in attributes_dict:\n",
    "                attributes_dict[attribute].append(character)\n",
    "            else:\n",
    "                attributes_dict[attribute] = [character]\n",
    "\n",
    "        attribute_list = [item['item']['category3Name']]\n",
    "\n",
    "        for i in attributes_dict:\n",
    "            if i not in ['용량', '구성', '']:\n",
    "                attribute_list = attribute_list + attributes_dict[i]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        dict_data = {\n",
    "            'ID': item['item']['id'],\n",
    "            '상품명': item['item']['productName'],\n",
    "            '상품 카테고리 대분류': item['item']['category1Name'],\n",
    "            '상품 카테고리 중분류': item['item']['category2Name'],\n",
    "            '상품 카테고리 소분류': item['item']['category3Name'],\n",
    "            '제조사': item['item']['maker'],\n",
    "            '브랜드': item['item']['brand'],\n",
    "            '특징': attribute_list\n",
    "        }\n",
    "        for attribute in attribute_list:\n",
    "            dict_data[attribute] = True\n",
    "\n",
    "        products_info.append(dict_data)\n",
    "        \n",
    "        # 딕셔너리의 리스트를 데이터프레임으로 변환합니다.\n",
    "    df = pd.DataFrame(products_info)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def makeCategoryDF(category_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : '1',\n",
    "        'pagingSize' : '40',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    total = data['props']['pageProps']['initialState']['products']['total']\n",
    "    total_page = math.ceil(total/40)\n",
    "    \n",
    "    df = makePageDF(category_id,1)\n",
    "    \n",
    "    if total_page >1:\n",
    "        for i in range(2,total_page+1):\n",
    "            new_df = makePageDF(category_id,i)\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['ID'])\n",
    "    df = pd.DataFrame(df).astype({'제조사': 'category'})\n",
    "    df = pd.DataFrame(df).astype({'브랜드': 'category'})\n",
    "    attribute_columns = df.columns.drop(['ID', '상품명', '상품 카테고리 대분류', '상품 카테고리 중분류','상품 카테고리 소분류','제조사','브랜드','특징'])\n",
    "    df[attribute_columns] = df[attribute_columns].fillna(False).astype('bool')\n",
    "    small_df = df.explode('특징')[['ID','상품명','특징']]\n",
    "    attributes = pd.DataFrame(small_df['특징'].unique()).reset_index()\n",
    "    attributes.columns = ['attribute_id', '특징']\n",
    "    attributes = pd.DataFrame(attributes).astype({'attribute_id': 'category'})\n",
    "    merged_df = pd.merge(small_df, attributes, on=['특징'])\n",
    "    \n",
    "    return df, merged_df, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71392ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, merged_df, attributes = makeCategoryDF(100001011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c0e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePageDF(category_id, page_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : str(page_id),\n",
    "        'pagingSize' : '80',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    \n",
    "    # 각 상품의 정보를 저장할 리스트를 생성합니다.\n",
    "    products_info = []\n",
    "\n",
    "    for item in itemList['list'][:80]:\n",
    "\n",
    "        attributes_dict = dict()\n",
    "        # 'attributeValue'와 'characterValue' 문자열을 리스트로 분리합니다.\n",
    "        attribute_values = item['item']['attributeValue'].split('|')\n",
    "        character_values = item['item']['characterValue'].split('|')\n",
    "\n",
    "        # 'attributeValue' 리스트의 각 요소에서 '_M' 문자열을 제거합니다.\n",
    "        attribute_values = [value.replace('_M', '') for value in attribute_values]\n",
    "\n",
    "        min_len = min(len(attribute_values), len(character_values))\n",
    "\n",
    "        for j in range(min_len):\n",
    "            attribute = attribute_values[j]\n",
    "            character = character_values[j]\n",
    "\n",
    "            if attribute in attributes_dict:\n",
    "                attributes_dict[attribute].append(character)\n",
    "            else:\n",
    "                attributes_dict[attribute] = [character]\n",
    "\n",
    "        attribute_list = [item['item']['category3Name']]\n",
    "\n",
    "        for i in attributes_dict:\n",
    "            if i not in ['용량', '구성', '']:\n",
    "                attribute_list = attribute_list + attributes_dict[i]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        dict_data = {\n",
    "            'ID': item['item']['id'],\n",
    "            '상품명': item['item']['productName'],\n",
    "            '상품 카테고리 대분류': item['item']['category1Name'],\n",
    "            '상품 카테고리 중분류': item['item']['category2Name'],\n",
    "            '상품 카테고리 소분류': item['item']['category3Name'],\n",
    "            '제조사': item['item']['maker'],\n",
    "            '브랜드': item['item']['brand'],\n",
    "            '특징': attribute_list\n",
    "        }\n",
    "        for attribute in attribute_list:\n",
    "            dict_data[attribute] = True\n",
    "\n",
    "        products_info.append(dict_data)\n",
    "        \n",
    "        # 딕셔너리의 리스트를 데이터프레임으로 변환합니다.\n",
    "    df = pd.DataFrame(products_info)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def makeCategoryDF(category_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : '1',\n",
    "        'pagingSize' : '80',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    total = data['props']['pageProps']['initialState']['products']['total']\n",
    "    total_page = math.ceil(total/80)\n",
    "    \n",
    "    df = makePageDF(category_id,1)\n",
    "    \n",
    "\n",
    "    if total_page >1:\n",
    "        for i in range(2,total_page+1):\n",
    "            new_df = makePageDF(category_id,i)\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['ID'])\n",
    "    df = pd.DataFrame(df).astype({'제조사': 'category'})\n",
    "    df = pd.DataFrame(df).astype({'브랜드': 'category'})\n",
    "    attribute_columns = df.columns.drop(['ID', '상품명', '상품 카테고리 대분류', '상품 카테고리 중분류','상품 카테고리 소분류','제조사','브랜드','특징'])\n",
    "    df[attribute_columns] = df[attribute_columns].fillna(False).astype('bool')\n",
    "    small_df = df.explode('특징')[['ID','상품명','특징']]\n",
    "    attributes = pd.DataFrame(small_df['특징'].unique()).reset_index()\n",
    "    attributes.columns = ['attribute_id', '특징']\n",
    "    attributes = pd.DataFrame(attributes).astype({'attribute_id': 'category'})\n",
    "    merged_df = pd.merge(small_df, attributes, on=['특징'])\n",
    "    \n",
    "    return df, merged_df, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, merged_df, attributes = makeCategoryDF(100001011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e34d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePageDF(category_id, page_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : str(page_id),\n",
    "        'pagingSize' : '60',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    \n",
    "    # 각 상품의 정보를 저장할 리스트를 생성합니다.\n",
    "    products_info = []\n",
    "\n",
    "    for item in itemList['list'][:60]:\n",
    "\n",
    "        attributes_dict = dict()\n",
    "        # 'attributeValue'와 'characterValue' 문자열을 리스트로 분리합니다.\n",
    "        attribute_values = item['item']['attributeValue'].split('|')\n",
    "        character_values = item['item']['characterValue'].split('|')\n",
    "\n",
    "        # 'attributeValue' 리스트의 각 요소에서 '_M' 문자열을 제거합니다.\n",
    "        attribute_values = [value.replace('_M', '') for value in attribute_values]\n",
    "\n",
    "        min_len = min(len(attribute_values), len(character_values))\n",
    "\n",
    "        for j in range(min_len):\n",
    "            attribute = attribute_values[j]\n",
    "            character = character_values[j]\n",
    "\n",
    "            if attribute in attributes_dict:\n",
    "                attributes_dict[attribute].append(character)\n",
    "            else:\n",
    "                attributes_dict[attribute] = [character]\n",
    "\n",
    "        attribute_list = [item['item']['category3Name']]\n",
    "\n",
    "        for i in attributes_dict:\n",
    "            if i not in ['용량', '구성', '']:\n",
    "                attribute_list = attribute_list + attributes_dict[i]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        dict_data = {\n",
    "            'ID': item['item']['id'],\n",
    "            '상품명': item['item']['productName'],\n",
    "            '상품 카테고리 대분류': item['item']['category1Name'],\n",
    "            '상품 카테고리 중분류': item['item']['category2Name'],\n",
    "            '상품 카테고리 소분류': item['item']['category3Name'],\n",
    "            '제조사': item['item']['maker'],\n",
    "            '브랜드': item['item']['brand'],\n",
    "            '특징': attribute_list\n",
    "        }\n",
    "        for attribute in attribute_list:\n",
    "            dict_data[attribute] = True\n",
    "\n",
    "        products_info.append(dict_data)\n",
    "        \n",
    "        # 딕셔너리의 리스트를 데이터프레임으로 변환합니다.\n",
    "    df = pd.DataFrame(products_info)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def makeCategoryDF(category_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : '1',\n",
    "        'pagingSize' : '60',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    total = data['props']['pageProps']['initialState']['products']['total']\n",
    "    total_page = math.ceil(total/60)\n",
    "    \n",
    "    df = makePageDF(category_id,1)\n",
    "    \n",
    "    if total_page >1:\n",
    "        for i in range(2,total_page+1):\n",
    "            new_df = makePageDF(category_id,i)\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['ID'])\n",
    "    df = pd.DataFrame(df).astype({'제조사': 'category'})\n",
    "    df = pd.DataFrame(df).astype({'브랜드': 'category'})\n",
    "    attribute_columns = df.columns.drop(['ID', '상품명', '상품 카테고리 대분류', '상품 카테고리 중분류','상품 카테고리 소분류','제조사','브랜드','특징'])\n",
    "    df[attribute_columns] = df[attribute_columns].fillna(False).astype('bool')\n",
    "    small_df = df.explode('특징')[['ID','상품명','특징']]\n",
    "    attributes = pd.DataFrame(small_df['특징'].unique()).reset_index()\n",
    "    attributes.columns = ['attribute_id', '특징']\n",
    "    attributes = pd.DataFrame(attributes).astype({'attribute_id': 'category'})\n",
    "    merged_df = pd.merge(small_df, attributes, on=['특징'])\n",
    "    \n",
    "    return df, merged_df, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a297e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, merged_df, attributes = makeCategoryDF(100001011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePageDF(category_id, page_id, pagingsize):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : str(page_id),\n",
    "        'pagingSize' : str(pagingsize),\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    \n",
    "    # 각 상품의 정보를 저장할 리스트를 생성합니다.\n",
    "    products_info = []\n",
    "\n",
    "    for item in itemList['list'][:pagingsize]:\n",
    "\n",
    "        attributes_dict = dict()\n",
    "        # 'attributeValue'와 'characterValue' 문자열을 리스트로 분리합니다.\n",
    "        attribute_values = item['item']['attributeValue'].split('|')\n",
    "        character_values = item['item']['characterValue'].split('|')\n",
    "\n",
    "        # 'attributeValue' 리스트의 각 요소에서 '_M' 문자열을 제거합니다.\n",
    "        attribute_values = [value.replace('_M', '') for value in attribute_values]\n",
    "\n",
    "        min_len = min(len(attribute_values), len(character_values))\n",
    "\n",
    "        for j in range(min_len):\n",
    "            attribute = attribute_values[j]\n",
    "            character = character_values[j]\n",
    "\n",
    "            if attribute in attributes_dict:\n",
    "                attributes_dict[attribute].append(character)\n",
    "            else:\n",
    "                attributes_dict[attribute] = [character]\n",
    "\n",
    "        attribute_list = [item['item']['category3Name']]\n",
    "\n",
    "        for i in attributes_dict:\n",
    "            if i not in ['용량', '구성', '']:\n",
    "                attribute_list = attribute_list + attributes_dict[i]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        dict_data = {\n",
    "            'ID': item['item']['id'],\n",
    "            '상품명': item['item']['productName'],\n",
    "            '상품 카테고리 대분류': item['item']['category1Name'],\n",
    "            '상품 카테고리 중분류': item['item']['category2Name'],\n",
    "            '상품 카테고리 소분류': item['item']['category3Name'],\n",
    "            '제조사': item['item']['maker'],\n",
    "            '브랜드': item['item']['brand'],\n",
    "            '특징': attribute_list\n",
    "        }\n",
    "        for attribute in attribute_list:\n",
    "            dict_data[attribute] = True\n",
    "\n",
    "        products_info.append(dict_data)\n",
    "        \n",
    "        # 딕셔너리의 리스트를 데이터프레임으로 변환합니다.\n",
    "    df = pd.DataFrame(products_info)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def makeCategoryDF(category_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : '1',\n",
    "        'pagingSize' : '80',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    total = data['props']['pageProps']['initialState']['products']['total']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for j in [20,40,60,80]:\n",
    "        total_page = math.ceil(total/j)\n",
    "        for i in range(1,total_page+1):\n",
    "            new_df = makePageDF(category_id,i,j)\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['ID'])\n",
    "    df = pd.DataFrame(df).astype({'제조사': 'category'})\n",
    "    df = pd.DataFrame(df).astype({'브랜드': 'category'})\n",
    "    attribute_columns = df.columns.drop(['ID', '상품명', '상품 카테고리 대분류', '상품 카테고리 중분류','상품 카테고리 소분류','제조사','브랜드','특징'])\n",
    "    df[attribute_columns] = df[attribute_columns].fillna(False).astype('bool')\n",
    "    small_df = df.explode('특징')[['ID','상품명','특징']]\n",
    "    attributes = pd.DataFrame(small_df['특징'].unique()).reset_index()\n",
    "    attributes.columns = ['attribute_id', '특징']\n",
    "    attributes = pd.DataFrame(attributes).astype({'attribute_id': 'category'})\n",
    "    merged_df = pd.merge(small_df, attributes, on=['특징'])\n",
    "    \n",
    "    return df, merged_df, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, merged_df, attributes = makeCategoryDF(100001011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePageDF(category_id, page_id, pagingsize):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : str(page_id),\n",
    "        'pagingSize' : str(pagingsize),\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    print(resp.request.url)\n",
    "    \n",
    "    # 각 상품의 정보를 저장할 리스트를 생성합니다.\n",
    "    products_info = []\n",
    "\n",
    "    for item in itemList['list'][:pagingsize]:\n",
    "\n",
    "        attributes_dict = dict()\n",
    "        # 'attributeValue'와 'characterValue' 문자열을 리스트로 분리합니다.\n",
    "        attribute_values = item['item']['attributeValue'].split('|')\n",
    "        character_values = item['item']['characterValue'].split('|')\n",
    "\n",
    "        # 'attributeValue' 리스트의 각 요소에서 '_M' 문자열을 제거합니다.\n",
    "        attribute_values = [value.replace('_M', '') for value in attribute_values]\n",
    "\n",
    "        min_len = min(len(attribute_values), len(character_values))\n",
    "\n",
    "        for j in range(min_len):\n",
    "            attribute = attribute_values[j]\n",
    "            character = character_values[j]\n",
    "\n",
    "            if attribute in attributes_dict:\n",
    "                attributes_dict[attribute].append(character)\n",
    "            else:\n",
    "                attributes_dict[attribute] = [character]\n",
    "\n",
    "        attribute_list = [item['item']['category3Name']]\n",
    "\n",
    "        for i in attributes_dict:\n",
    "            if i not in ['용량', '구성', '']:\n",
    "                attribute_list = attribute_list + attributes_dict[i]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        dict_data = {\n",
    "            'ID': item['item']['id'],\n",
    "            '상품명': item['item']['productName'],\n",
    "            '상품 카테고리 대분류': item['item']['category1Name'],\n",
    "            '상품 카테고리 중분류': item['item']['category2Name'],\n",
    "            '상품 카테고리 소분류': item['item']['category3Name'],\n",
    "            '제조사': item['item']['maker'],\n",
    "            '브랜드': item['item']['brand'],\n",
    "            '특징': attribute_list\n",
    "        }\n",
    "        for attribute in attribute_list:\n",
    "            dict_data[attribute] = True\n",
    "\n",
    "        products_info.append(dict_data)\n",
    "        \n",
    "        # 딕셔너리의 리스트를 데이터프레임으로 변환합니다.\n",
    "    df = pd.DataFrame(products_info)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def makeCategoryDF(category_id):\n",
    "    url = 'https://search.shopping.naver.com/search/category/' + str(category_id)\n",
    "    params = {\n",
    "        'pagingIndex' : '1',\n",
    "        'pagingSize' : '80',\n",
    "        'productSet' : 'model'\n",
    "    }\n",
    "    headers = {'User-Agent':generate_user_agent(os =('mac', 'linux'),device_type='desktop')}\n",
    "    resp = request('GET',url=url, params=params, headers=headers)\n",
    "    dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "    script_tag = dom.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    data = json.loads(script_tag.text)\n",
    "    itemList = data['props']['pageProps']['initialState']['products']\n",
    "    total = data['props']['pageProps']['initialState']['products']['total']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for j in [60,80]:\n",
    "        total_page = math.ceil(total/j)\n",
    "        for i in range(1,total_page+1):\n",
    "            new_df = makePageDF(category_id,i,j)\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['ID'])\n",
    "    df = pd.DataFrame(df).astype({'제조사': 'category'})\n",
    "    df = pd.DataFrame(df).astype({'브랜드': 'category'})\n",
    "    attribute_columns = df.columns.drop(['ID', '상품명', '상품 카테고리 대분류', '상품 카테고리 중분류','상품 카테고리 소분류','제조사','브랜드','특징'])\n",
    "    df[attribute_columns] = df[attribute_columns].fillna(False).astype('bool')\n",
    "    small_df = df.explode('특징')[['ID','상품명','특징']]\n",
    "    attributes = pd.DataFrame(small_df['특징'].unique()).reset_index()\n",
    "    attributes.columns = ['attribute_id', '특징']\n",
    "    attributes = pd.DataFrame(attributes).astype({'attribute_id': 'category'})\n",
    "    merged_df = pd.merge(small_df, attributes, on=['특징'])\n",
    "    \n",
    "    return df, merged_df, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8024005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, merged_df, attributes = makeCategoryDF(100001011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e472d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsg",
   "language": "python",
   "name": "dsg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
